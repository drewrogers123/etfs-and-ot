{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f616e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Prevents'NoneType' object has no attribute 'split' error\n",
    "os.environ['OMP_NUM_THREADS'] = '1' \n",
    "\n",
    "# configurations\n",
    "FILE_PATH = 'your_file.csv'  # Path to your CSV file\n",
    "K_MAX = 15 \n",
    "N_INIT = 10  # Number of initializations to run for better results\n",
    "MAX_ITER = 100\n",
    "\n",
    "# pytorch k-means implementation\n",
    "\n",
    "class KMeansTorch:\n",
    "    \"\"\"K-Means clustering implemented using PyTorch tensors.\"\"\"\n",
    "    def __init__(self, n_clusters, max_iter=100, n_init=10, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        self.inertia_ = None\n",
    "        self.cluster_centers_ = None\n",
    "\n",
    "    def _initialize_centroids(self, X):\n",
    "        \"\"\"Randomly selects initial centroids from the data points.\"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        rand_indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
    "        return X[rand_indices]\n",
    "\n",
    "    def _fit_single(self, X):\n",
    "        \"\"\"Performs a single run of the K-Means algorithm.\"\"\"\n",
    "        # Initialize centroids\n",
    "        centroids = self._initialize_centroids(X).clone().to(X.device)\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            # E-Step: Compute distances and assign clusters\n",
    "            # Compute squared Euclidean distances\n",
    "            distances = torch.sum((X.unsqueeze(1) - centroids.unsqueeze(0))**2, dim=2)\n",
    "            \n",
    "            # Assign clusters based on closest centroid\n",
    "            cluster_assignments = torch.argmin(distances, dim=1)\n",
    "            \n",
    "            # M-Step: Update centroids\n",
    "            new_centroids = torch.zeros_like(centroids)\n",
    "            counts = torch.zeros(self.n_clusters, dtype=torch.int64)\n",
    "            \n",
    "            # Using scatter_add to sum points in each cluster\n",
    "            new_centroids.scatter_add_(0, cluster_assignments.view(-1, 1).repeat(1, X.shape[1]), X)\n",
    "        \n",
    "            counts = torch.bincount(cluster_assignments, minlength=self.n_clusters)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            valid_counts = counts.view(-1, 1).clamp(min=1).float()\n",
    "            new_centroids /= valid_counts\n",
    "            \n",
    "            # Check for convergence\n",
    "            if torch.allclose(centroids, new_centroids):\n",
    "                break\n",
    "                \n",
    "            centroids = new_centroids\n",
    "\n",
    "        # Calculate inertia (WCSS)\n",
    "        final_distances = torch.sum((X.unsqueeze(1) - centroids.unsqueeze(0))**2, dim=2)\n",
    "        inertia = final_distances.min(dim=1)[0].sum().item()\n",
    "        \n",
    "        return inertia, centroids\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Runs K-Means multiple times and selects the best result.\"\"\"\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        best_inertia = float('inf')\n",
    "        best_centroids = None\n",
    "        \n",
    "        for init in range(self.n_init):\n",
    "            # Set different random state for each initialization\n",
    "            self.random_state = 42 + init \n",
    "            inertia, centroids = self._fit_single(X_tensor)\n",
    "            \n",
    "            if inertia < best_inertia:\n",
    "                best_inertia = inertia\n",
    "                best_centroids = centroids\n",
    "                \n",
    "        self.inertia_ = best_inertia\n",
    "        self.cluster_centers_ = best_centroids.numpy()\n",
    "        return self\n",
    "\n",
    "# Load Data and Prepare for Clustering\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '{FILE_PATH}' was not found.\")\n",
    "    exit()\n",
    "\n",
    "weight_cols = [col for col in df.columns if col.endswith('_weight')]\n",
    "if not weight_cols:\n",
    "    print(\"Error: No columns ending with '_weight' were found in the CSV.\")\n",
    "    exit()\n",
    "    \n",
    "X = df[weight_cols].values\n",
    "print(f\"Successfully loaded {len(X)} rows with {len(weight_cols)} weight features.\")\n",
    "\n",
    "wcss = []\n",
    "K_range = range(2, K_MAX + 1)\n",
    "\n",
    "print(f\"Calculating WCSS for K = 2 to {K_MAX} using {N_INIT} initializations...\")\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeansTorch(n_clusters=k, n_init=N_INIT, max_iter=MAX_ITER)\n",
    "    kmeans.fit(X) \n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, wcss, marker='o', linestyle='-', color='darkcyan')\n",
    "plt.title(f'Elbow Method for Optimal K (PyTorch Implementation)', fontsize=16)\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(K_range)\n",
    "\n",
    "print(\"\\nPlotting results. Look for the 'elbow' where the curve bends sharply.\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"Select the optimal K from the Elbow plot and run the final K-MeansTorch model with that K.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6654192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete. Identified 9 portfolio regimes.\n",
      "Total inertia (WCSS) for K=9: 0.9411\n",
      "\n",
      "--- Portfolio Regime Analysis (K=9) ---\n",
      "The most common regime is Cluster 1, observed on 381.0 days.\n",
      "--------------------------------------------------\n",
      "\n",
      "**Most Common Portfolio Regime Weights**\n",
      "(Cluster 1 Centroid - Weights in %):\n",
      "----------------------------------------\n",
      "XLK: 18.33%\n",
      "XLF: 13.44%\n",
      "XLV: 16.78%\n",
      "XLY: 6.53%\n",
      "XLP: 7.10%\n",
      "XLE: 16.24%\n",
      "XLI: 5.87%\n",
      "XLU: 6.69%\n",
      "XLRE: 2.06%\n",
      "XLB: 2.47%\n",
      "XLC: 4.49%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Prevents'NoneType' object has no attribute 'split' error\n",
    "os.environ['OMP_NUM_THREADS'] = '1' \n",
    "\n",
    "# configurations\n",
    "FILE_PATH = '/Users/drewrogers/Desktop/ETFsAndOT/etf_nav_and_weights.csv'\n",
    "K_FINAL = 9         # The chosen number of clusters (regimes)\n",
    "N_INIT = 10         # Number of initializations\n",
    "MAX_ITER = 100\n",
    "\n",
    "# PyTorch K-Means implementation\n",
    "\n",
    "class KMeansTorch:\n",
    "    \"\"\"K-Means clustering implemented using PyTorch tensors.\"\"\"\n",
    "    def __init__(self, n_clusters, max_iter=100, n_init=10, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        self.inertia_ = None\n",
    "        self.cluster_centers_ = None\n",
    "        self.labels_ = None # To store cluster assignments\n",
    "\n",
    "    def _initialize_centroids(self, X):\n",
    "        \"\"\"Randomly selects initial centroids from the data points.\"\"\"\n",
    "        # Using numpy for random selection\n",
    "        np.random.seed(self.random_state)\n",
    "        rand_indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
    "        return X[rand_indices]\n",
    "\n",
    "    def _fit_single(self, X):\n",
    "        \"\"\"Performs a single run of the K-Means algorithm.\"\"\"\n",
    "        centroids = self._initialize_centroids(X).clone().to(X.device)\n",
    "        cluster_assignments = None\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # E-Step: Assign clusters based on closest centroid\n",
    "            # Compute squared Euclidean distances\n",
    "            distances = torch.sum((X.unsqueeze(1) - centroids.unsqueeze(0))**2, dim=2)\n",
    "            cluster_assignments = torch.argmin(distances, dim=1)\n",
    "            \n",
    "            # M-Step: Update centroids\n",
    "            new_centroids = torch.zeros_like(centroids)\n",
    "            \n",
    "            # Use scatter_add_ to sum all points belonging to a cluster\n",
    "            new_centroids.scatter_add_(0, cluster_assignments.view(-1, 1).repeat(1, X.shape[1]), X)\n",
    "            \n",
    "            # Count the number of points in each cluster\n",
    "            counts = torch.bincount(cluster_assignments, minlength=self.n_clusters)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            valid_counts = counts.view(-1, 1).clamp(min=1).float()\n",
    "            new_centroids /= valid_counts\n",
    "            \n",
    "            # Check for convergence\n",
    "            if torch.allclose(centroids, new_centroids):\n",
    "                break\n",
    "                \n",
    "            centroids = new_centroids\n",
    "\n",
    "        # Calculate inertia (WCSS)\n",
    "        final_distances = torch.sum((X.unsqueeze(1) - centroids.unsqueeze(0))**2, dim=2)\n",
    "        inertia = final_distances.min(dim=1)[0].sum().item()\n",
    "        \n",
    "        return inertia, centroids, cluster_assignments\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Runs K-Means multiple times and selects the best result.\"\"\"\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        best_inertia = float('inf')\n",
    "        \n",
    "        for init in range(self.n_init):\n",
    "            self.random_state = 42 + init \n",
    "            inertia, centroids, assignments = self._fit_single(X_tensor)\n",
    "            \n",
    "            if inertia < best_inertia:\n",
    "                best_inertia = inertia\n",
    "                self.cluster_centers_ = centroids.numpy()\n",
    "                self.labels_ = assignments.numpy()\n",
    "                \n",
    "        self.inertia_ = best_inertia\n",
    "        return self.labels_\n",
    "\n",
    "\n",
    "# Load Data and Prepare for Clustering\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '{FILE_PATH}' was not found. Please double-check the path.\")\n",
    "    exit()\n",
    "\n",
    "weight_cols = [col for col in df.columns if col.endswith('_weight')]\n",
    "if not weight_cols:\n",
    "    print(\"Error: No columns ending with '_weight' were found in the CSV.\")\n",
    "    exit()\n",
    "    \n",
    "X = df[weight_cols].values\n",
    "\n",
    "# Run the final K-Means model with K=9\n",
    "kmeans_final = KMeansTorch(n_clusters=K_FINAL, n_init=N_INIT, max_iter=MAX_ITER)\n",
    "cluster_labels = kmeans_final.fit_predict(X)\n",
    "\n",
    "print(f\"Clustering complete. Identified {K_FINAL} portfolio regimes.\")\n",
    "print(f\"Total inertia (WCSS) for K={K_FINAL}: {kmeans_final.inertia_:.4f}\\n\")\n",
    "\n",
    "\n",
    "# summarize and Identify Most Common State\n",
    "\n",
    "centers_df = pd.DataFrame(kmeans_final.cluster_centers_, columns=weight_cols)\n",
    "centers_df.index.name = 'Cluster'\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "# Calculate the size (frequency) of each regime\n",
    "cluster_sizes = df.groupby('Cluster').size().rename('Size (Days)')\n",
    "summary = centers_df.join(cluster_sizes)\n",
    "\n",
    "# Identify the most frequent regime\n",
    "most_common_id = summary['Size (Days)'].idxmax()\n",
    "most_common_state = summary.loc[most_common_id]\n",
    "\n",
    "# Print the summary and most common regime details\n",
    "\n",
    "print(f\"--- Portfolio Regime Analysis (K={K_FINAL}) ---\")\n",
    "print(f\"The most common regime is Cluster {most_common_id}, observed on {most_common_state['Size (Days)']} days.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n**Most Common Portfolio Regime Weights**\")\n",
    "print(f\"(Cluster {most_common_id} Centroid - Weights in %):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Extract and format the weights for the largest cluster's centroid\n",
    "most_common_weights = most_common_state[weight_cols] * 100\n",
    "most_common_weights.index = [col.replace('_weight', '').upper() for col in weight_cols]\n",
    "\n",
    "# Print weights in the requested format\n",
    "for etf, weight in most_common_weights.items():\n",
    "    print(f\"{etf}: {weight:.2f}%\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
